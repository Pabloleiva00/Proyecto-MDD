{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from PIL import Image\n",
    "from ast import literal_eval\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carga datos Train-Test-Valid**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/birds.csv\")\n",
    "train = dataset.loc[dataset['data set'] == \"train\"].head(1100).reset_index()\n",
    "test = dataset.loc[dataset['data set'] == \"test\"].reset_index()\n",
    "valid = dataset.loc[dataset['data set'] == \"valid\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['index'])\n",
    "test = test.drop(columns=['index'])\n",
    "valid = valid.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class id</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>labels</th>\n",
       "      <th>data set</th>\n",
       "      <th>scientific name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>524.0</td>\n",
       "      <td>test/BLACK BREASTED PUFFBIRD/3.jpg</td>\n",
       "      <td>BLACK BREASTED PUFFBIRD</td>\n",
       "      <td>test</td>\n",
       "      <td>NOTHARCHUS PECTORALIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621</th>\n",
       "      <td>524.0</td>\n",
       "      <td>test/BLACK BREASTED PUFFBIRD/4.jpg</td>\n",
       "      <td>BLACK BREASTED PUFFBIRD</td>\n",
       "      <td>test</td>\n",
       "      <td>NOTHARCHUS PECTORALIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>524.0</td>\n",
       "      <td>test/BLACK BREASTED PUFFBIRD/1.jpg</td>\n",
       "      <td>BLACK BREASTED PUFFBIRD</td>\n",
       "      <td>test</td>\n",
       "      <td>NOTHARCHUS PECTORALIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>524.0</td>\n",
       "      <td>test/BLACK BREASTED PUFFBIRD/2.jpg</td>\n",
       "      <td>BLACK BREASTED PUFFBIRD</td>\n",
       "      <td>test</td>\n",
       "      <td>NOTHARCHUS PECTORALIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>524.0</td>\n",
       "      <td>test/BLACK BREASTED PUFFBIRD/5.jpg</td>\n",
       "      <td>BLACK BREASTED PUFFBIRD</td>\n",
       "      <td>test</td>\n",
       "      <td>NOTHARCHUS PECTORALIS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class id                           filepaths                   labels  \\\n",
       "2620     524.0  test/BLACK BREASTED PUFFBIRD/3.jpg  BLACK BREASTED PUFFBIRD   \n",
       "2621     524.0  test/BLACK BREASTED PUFFBIRD/4.jpg  BLACK BREASTED PUFFBIRD   \n",
       "2622     524.0  test/BLACK BREASTED PUFFBIRD/1.jpg  BLACK BREASTED PUFFBIRD   \n",
       "2623     524.0  test/BLACK BREASTED PUFFBIRD/2.jpg  BLACK BREASTED PUFFBIRD   \n",
       "2624     524.0  test/BLACK BREASTED PUFFBIRD/5.jpg  BLACK BREASTED PUFFBIRD   \n",
       "\n",
       "     data set        scientific name  \n",
       "2620     test  NOTHARCHUS PECTORALIS  \n",
       "2621     test  NOTHARCHUS PECTORALIS  \n",
       "2622     test  NOTHARCHUS PECTORALIS  \n",
       "2623     test  NOTHARCHUS PECTORALIS  \n",
       "2624     test  NOTHARCHUS PECTORALIS  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABBOTTS BOOBY                 180\n",
       "ABYSSINIAN GROUND HORNBILL    179\n",
       "ABBOTTS BABBLER               163\n",
       "AFRICAN EMERALD CUCKOO        154\n",
       "AFRICAN OYSTER CATCHER        152\n",
       "AFRICAN FIREFINCH             137\n",
       "AFRICAN CROWNED CRANE         135\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos no están muy balanceados, ya que hay especies con más fotos que otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Hot Encoding de las variables objetivo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"ABBOTTS BOOBY\", \"ABYSSINIAN GROUND HORNBILL\", \"ABBOTTS BABBLER\", \"AFRICAN EMERALD CUCKOO\", \"AFRICAN OYSTER CATCHER\", \"AFRICAN FIREFINCH\", \"AFRICAN CROWNED CRANE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "for i in train.index:\n",
    "    label = train[\"labels\"][i]\n",
    "    path = \"data/\" + train[\"filepaths\"][i]\n",
    "    image = Image.open(path)\n",
    "    array = np.array(image)\n",
    "    X_train.append(array)\n",
    "    Y_train.append(label)\n",
    "X_train = np.array(X_train)/255\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(35):\n",
    "    label = test[\"labels\"][i]\n",
    "    path = \"data/\" + test[\"filepaths\"][i]\n",
    "    image = Image.open(path)\n",
    "    array = np.array(image)\n",
    "    X_test.append(array)\n",
    "    Y_test.append(label)\n",
    "X_test = np.array(X_test)/255\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = []\n",
    "Y_valid = []\n",
    "for i in range(35):\n",
    "    label = valid[\"labels\"][i]\n",
    "    path = \"data/\" + valid[\"filepaths\"][i]\n",
    "    image = Image.open(path)\n",
    "    array = np.array(image)\n",
    "    X_valid.append(array)\n",
    "    Y_valid.append(label)\n",
    "X_valid = np.array(X_valid)/255\n",
    "Y_valid = np.array(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OrdinalEncoder(categories=[labels])\n",
    "Y_train = enc.fit_transform(Y_train.reshape(-1, 1)).ravel()\n",
    "Y_test = enc.fit_transform(Y_test.reshape(-1, 1)).ravel()\n",
    "Y_valid = enc.fit_transform(Y_valid.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación del modelo CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10, # Degree range for random rotations.\n",
    "    zoom_range=0.1, # Range for random zoom\n",
    "    width_shift_range=0.1, # fraction of total width (traslation)\n",
    "    height_shift_range=0.1, # fraction of total height (traslation)\n",
    "    horizontal_flip=False, # Randomly flip inputs horizontally.\n",
    "    vertical_flip=False # Randomly flip inputs vertically.\n",
    ")\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "input_shape=(224, 224, 3)\n",
    "\n",
    "# Convolution layer 1\n",
    "model.add(Conv2D(filters=120, kernel_size=(3,3), strides=(1,1),padding='same',activation='relu',input_shape=input_shape))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Convolution layer 2\n",
    "model.add(Conv2D(filters=100, kernel_size=(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Convolution layer 3\n",
    "model.add(Conv2D(filters=80, kernel_size=(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Convolution layer 4\n",
    "model.add(Conv2D(filters=60, kernel_size=(3,3),strides=(1,1),padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(units=1000,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(units=1000,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1000,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=len(labels) ,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    datagen.flow(X_train, Y_train, batch_size=64),\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    verbose=2,\n",
    "    steps_per_epoch=X_train.shape[0] // 64,\n",
    "    callbacks=[learning_rate_reduction]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, Y_train)\n",
    "print(\"Final training loss: {:.8f}\".format(loss))\n",
    "print(\"Final training accuracy: {:.4%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo resultó ser bastante malo, por lo que buscaremos los mejores hiperparámetros posibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    initializer = initializers.HeNormal(seed=20)\n",
    "    model = Sequential()    \n",
    "    regu = hp.Choice('l1_reg', [0.001, 0.0001])\n",
    "    input_shape=(200, 200, 3)\n",
    "    \n",
    "    #Conv layer loop\n",
    "    for i in range(4):\n",
    "        nodes = hp.Choice(f'nodes_{i+1}', [32, 64, 128, 256, 512])\n",
    "        model.add(Conv2D(nodes, (3, 3), activation='relu', input_shape=input_shape, kernel_initializer=initializer, kernel_regularizer=regularizers.l1(regu)))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(BatchNormalization(momentum=hp.Float('momentum', min_value=0.8, max_value=0.9, step=0.05), epsilon=0.001))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Fully Connected layers\n",
    "    for i in range(4):\n",
    "        units = hp.Choice(f'units_{i+1}', [32, 64, 128, 256, 512])\n",
    "        model.add(Dense(units, activation='relu',kernel_initializer=initializer,kernel_regularizer=regularizers.l1(regu)))\n",
    "        model.add(BatchNormalization(momentum=hp.Float('momentum', min_value=0.8, max_value=0.95, step=0.05), epsilon=0.001))\n",
    "     \n",
    "    model.add(Dense(5, activation='softmax', kernel_initializer=initializer))\n",
    "\n",
    "    opt = Adam(learning_rate=hp.Choice('learning_rate', [0.001, 0.0001]))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a bayesian optimizer\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=keras_tuner.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=70, \n",
    "    num_initial_points=35,  \n",
    "    overwrite=True  \n",
    ")\n",
    "tuner.search(train_main, epochs=5, validation_data=(valid_main))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_hp.values.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Print the best model summary\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = initializers.HeNormal(seed=20)\n",
    "model1 = Sequential()\n",
    "    \n",
    "for i in range(4):        \n",
    "    \n",
    "    model1.add(Conv2D(best_hp.get(f'nodes_{i+1}'), (3, 3), activation='relu', kernel_initializer=initializer, kernel_regularizer=regularizers.l1(best_hp.get('l1_reg'))))\n",
    "    model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model1.add(BatchNormalization(momentum=best_hp.get(f'momentum'), epsilon=0.001))\n",
    "    \n",
    "model1.add(Flatten())\n",
    "    \n",
    "for i in range(4):\n",
    "    model1.add(Dense(best_hp.get(f'units_{i+1}'),activation='relu',kernel_initializer=initializer,kernel_regularizer=regularizers.l1(best_hp.get('l1_reg'))))\n",
    "    model1.add(BatchNormalization(momentum=best_hp.get(f'momentum'), epsilon=0.001))\n",
    "        \n",
    "model1.add(Dense(5, activation='softmax', kernel_initializer=initializer))\n",
    "\n",
    "opt = Adam(learning_rate=best_hp.get('learning_rate'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_hist = model1.fit(train_main, steps_per_epoch = len(train_main), epochs = 8, validation_data=valid_main)\n",
    "# Evaluate the model\n",
    "loss, accuracy = model1.evaluate(test_main)\n",
    "print(\"Final training loss: {:.8f}\".format(loss))\n",
    "print(\"Final training accuracy: {:.4%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
